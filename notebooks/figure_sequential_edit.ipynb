{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021, InterDigital R&D France. All rights reserved.\n",
    "#\n",
    "# This source code is made available under the license found in the\n",
    "# LICENSE.txt in the root directory of this source tree.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import yaml\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "import sys\n",
    "\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    sys.path.append('..')\n",
    "    os.chdir('..')\n",
    "\n",
    "from datasets import *\n",
    "from trainer import *\n",
    "from utils.functions import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "device = torch.device('cuda')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='001', help='Path to the config file.')\n",
    "parser.add_argument('--attr', type=str, default='Eyeglasses', help='attribute for manipulation.')\n",
    "parser.add_argument('--latent_path', type=str, default='./data/celebahq_dlatents_psp.npy', help='dataset path')\n",
    "parser.add_argument('--label_file', type=str, default='./data/celebahq_anno.npy', help='label file path')\n",
    "parser.add_argument('--stylegan_model_path', type=str, default='./pixel2style2pixel/pretrained_models/psp_ffhq_encode.pt', help='stylegan model path')\n",
    "parser.add_argument('--classifier_model_path', type=str, default='./models/latent_classifier_epoch_20.pth', help='pretrained attribute classifier')\n",
    "parser.add_argument('--log_path', type=str, default='./logs/', help='log file path')\n",
    "opts = parser.parse_args([])\n",
    "\n",
    "# Celeba attribute list\n",
    "attr_dict = {'5_o_Clock_Shadow': 0, 'Arched_Eyebrows': 1, 'Attractive': 2, 'Bags_Under_Eyes': 3, \\\n",
    "            'Bald': 4, 'Bangs': 5, 'Big_Lips': 6, 'Big_Nose': 7, 'Black_Hair': 8, 'Blond_Hair': 9, \\\n",
    "            'Blurry': 10, 'Brown_Hair': 11, 'Bushy_Eyebrows': 12, 'Chubby': 13, 'Double_Chin': 14, \\\n",
    "            'Eyeglasses': 15, 'Goatee': 16, 'Gray_Hair': 17, 'Heavy_Makeup': 18, 'High_Cheekbones': 19, \\\n",
    "            'Male': 20, 'Mouth_Slightly_Open': 21, 'Mustache': 22, 'Narrow_Eyes': 23, 'No_Beard': 24, \\\n",
    "            'Oval_Face': 25, 'Pale_Skin': 26, 'Pointy_Nose': 27, 'Receding_Hairline': 28, 'Rosy_Cheeks': 29, \\\n",
    "            'Sideburns': 30, 'Smiling': 31, 'Straight_Hair': 32, 'Wavy_Hair': 33, 'Wearing_Earrings': 34, \\\n",
    "            'Wearing_Hat': 35, 'Wearing_Lipstick': 36, 'Wearing_Necklace': 37, 'Wearing_Necktie': 38, 'Young': 39}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer model.\n",
    "log_dir = os.path.join(opts.log_path, opts.config) + '/'\n",
    "config = yaml.load(open('./configs/' + opts.config + '.yaml', 'r'))\n",
    "\n",
    "trainer = Trainer(config, None, None, opts.label_file)\n",
    "trainer.initialize(opts.stylegan_model_path, opts.classifier_model_path)   \n",
    "trainer.to(device)\n",
    "print('Load model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1. Teaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "testdata_dir = './data/teaser/'\n",
    "\n",
    "teaser_attrs = [{'Smiling':1, 'Bangs':1, 'Arched_Eyebrows':1, 'Young':-1}, \\\n",
    "                {'Young':1, 'Smiling':1, 'No_Beard':-1, 'Eyeglasses':1}  ]\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for k, idx in enumerate([1,2]):\n",
    "\n",
    "        w_0 = np.load(testdata_dir + 'latent_code_%05d.npy'%idx)\n",
    "        w_0 = torch.tensor(w_0).to(device)\n",
    "        \n",
    "        x_0 = img_to_tensor(Image.open(testdata_dir + '%05d.jpg'%idx))\n",
    "        x_0 = x_0.unsqueeze(0).to(device)\n",
    "        img_l = [x_0] # original image\n",
    "        \n",
    "        x_1, _ = trainer.StyleGAN([w_0], input_is_latent=True, randomize_noise=False)\n",
    "        x_0 = torch.ones((x_1.size(0), x_1.size(1), x_1.size(2),x_1.size(3)+40)).type_as(x_1)\n",
    "        x_0[:,:,:,20:1044] = x_1[:,:,:,:]\n",
    "        img_l.append(x_0) # projected image\n",
    "        \n",
    "        w_1 = w_0\n",
    "        attrs = teaser_attrs[k]\n",
    "        for attr in list(attrs.keys()):\n",
    "            \n",
    "            trainer.attr_num = attr_dict[attr]\n",
    "            trainer.load_model(log_dir)\n",
    "            \n",
    "            alpha = torch.tensor(1.0) * attrs[attr]\n",
    "            w_1 = trainer.T_net(w_1.view(w_0.size(0), -1), alpha.unsqueeze(0).to(device))\n",
    "            w_1 = w_1.view(w_0.size())\n",
    "            w_1 = torch.cat((w_1[:,:11,:], w_0[:,11:,:]), 1)\n",
    "            x_1, _ = trainer.StyleGAN([w_1], input_is_latent=True, randomize_noise=False)\n",
    "            img_l.append(x_1.data)\n",
    "\n",
    "        img = img_l[0] if len(img_l)==1 else torch.cat(img_l, 3)\n",
    "        img = np.clip(clip_img(img)[0].cpu().numpy()*255.,0,255).astype(np.uint8)\n",
    "        img = Image.fromarray(img.transpose(1,2,0))\n",
    "        plt.figure(figsize=(30,5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4. Sequential facial attribute editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_dir = './data/test/'\n",
    "\n",
    "attrs_list = [{'Chubby':-0.5, 'Blond_Hair':1.5, 'Smiling':1, 'Wearing_Lipstick':1, 'Eyeglasses':1.5}, \\\n",
    "                {'Eyeglasses':-1.5, 'Bangs':1, 'Bags_Under_Eyes':1, 'Smiling':-1, 'Young':-1}, \\\n",
    "                {'Smiling':1, 'No_Beard':-1, 'Receding_Hairline':1, 'Eyeglasses':1, 'Arched_Eyebrows':1}, \\\n",
    "                {'Smiling':-1, 'Chubby':-0.5, 'Goatee':1, 'Eyeglasses':1, 'Pale_Skin':1}  ]\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for k, idx in enumerate([4,5,6,7]):\n",
    "\n",
    "        w_0 = np.load(testdata_dir + 'latent_code_%05d.npy'%idx)\n",
    "        w_0 = torch.tensor(w_0).to(device)\n",
    "        \n",
    "        x_0 = img_to_tensor(Image.open(testdata_dir + '%05d.jpg'%idx))\n",
    "        x_0 = x_0.unsqueeze(0).to(device)\n",
    "        img_l = [x_0] # original image\n",
    "        \n",
    "        x_1, _ = trainer.StyleGAN([w_0], input_is_latent=True, randomize_noise=False)\n",
    "        x_0 = torch.ones((x_1.size(0), x_1.size(1), x_1.size(2),x_1.size(3)+40)).type_as(x_1)\n",
    "        x_0[:,:,:,20:1044] = x_1[:,:,:,:]\n",
    "        img_l.append(x_0) # projected image\n",
    "        \n",
    "        w_1 = w_0\n",
    "        attrs = attrs_list[k]\n",
    "        for attr in list(attrs.keys()):\n",
    "            \n",
    "            trainer.attr_num = attr_dict[attr]\n",
    "            trainer.load_model(log_dir)\n",
    "            \n",
    "            alpha = torch.tensor(1.0) * attrs[attr]\n",
    "            w_1 = trainer.T_net(w_1.view(w_0.size(0), -1), alpha.unsqueeze(0).to(device))\n",
    "            w_1 = w_1.view(w_0.size())\n",
    "            w_1 = torch.cat((w_1[:,:11,:], w_0[:,11:,:]), 1)\n",
    "            x_1, _ = trainer.StyleGAN([w_1], input_is_latent=True, randomize_noise=False)\n",
    "            img_l.append(x_1.data)\n",
    "\n",
    "        img = img_l[0] if len(img_l)==1 else torch.cat(img_l, 3)\n",
    "        img = np.clip(clip_img(img)[0].cpu().numpy()*255.,0,255).astype(np.uint8)\n",
    "        img = Image.fromarray(img.transpose(1,2,0))\n",
    "        plt.figure(figsize=(30,5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
